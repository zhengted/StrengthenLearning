# 贝尔曼期望方程推导 (Bellman Expectation Equation Derivation)

我们将从马尔可夫决策过程（Markov Decision Process, MDP）的基本定义出发，一步步推导贝尔曼期望方程。

### 1. 马尔可夫决策过程 (MDP)

一个 MDP 由一个五元组 $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$ 组成：

-   **$\mathcal{S}$**: **状态空间 (State Space)**，表示智能体可能所处的所有状态的集合。
-   **$\mathcal{A}$**: **动作空间 (Action Space)**，表示智能体可以执行的所有动作的集合。
-   **$P$**: **状态转移概率 (State Transition Probability)**。$P(s' | s, a)$ 表示在状态 $s$ 下执行动作 $a$ 后，转移到状态 $s'$ 的概率。
-   **$R$**: **奖励函数 (Reward Function)**。$R(s, a, s')$ 表示在状态 $s$ 执行动作 $a$ 转移到状态 $s'$ 后，智能体获得的即时奖励。
-   **$\gamma$**: **折扣因子 (Discount Factor)**，$\gamma \in [0, 1]$。它用于衡量未来奖励相对于当前奖励的重要性。

### 2. 策略 (Policy)

**策略 ($\pi$)** 是智能体的行为准则，它定义了在给定状态下选择每个动作的概率。
-   $\pi(a | s)$: 表示在状态 $s$ 下，选择执行动作 $a$ 的概率。

### 3. 价值函数 (Value Function)

#### a. 状态价值函数 (State-Value Function) $V^{\pi}(s)$

它表示从状态 $s$ 开始，并始终遵循策略 $\pi$，所能获得的**期望总折扣奖励**。

$V^{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s]$

其中，$G_t$ 是从时间步 $t$ 开始的总折扣奖励 (Total Discounted Return)：
$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$

#### b. 动作价值函数 (Action-Value Function) $Q^{\pi}(s, a)$

它表示在状态 $s$ 下，执行一个特定的动作 $a$，然后继续遵循策略 $\pi$，所能获得的**期望总折扣奖励**。

$Q^{\pi}(s, a) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]$

#### c. Vπ(s) 和 Qπ(s,a) 的关系

您提出的关于这两个价值函数关系的问题非常关键。它们之间的关系可以总结为：

**状态价值函数 $V^{\pi}(s)$ 是动作价值函数 $Q^{\pi}(s, a)$ 在该状态下关于动作 $a$ 的期望值。**

换句话说，$V^{\pi}(s)$ 是 $Q^{\pi}(s, a)$ 基于策略 $\pi$ 的加权平均。这个权重就是策略 $\pi(a|s)$ ―― 即在状态 $s$ 下选择动作 $a$ 的概率。

这个关系可以用以下公式精确表示：

$V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q^{\pi}(s, a)$

这个公式直观地告诉我们：一个状态的价值，等于在该状态下遵循策略 $\pi$ 去尝试所有不同动作所能带来的价值的平均值。

### 对 $V^{\pi}(s)$ 定义的进一步说明

您对 $V^{\pi}(s)$ 的定义提出了很好的问题，这里我们做两点补充说明：

#### 1. 期望 $\mathbb{E}_{\pi}$ 如何展开？

期望 $\mathbb{E}_{\pi}[G_t | S_t = s]$ 是对从状态 $s$ 出发，遵循策略 $\pi$ 所能产生的所有**完整轨迹 (trajectories)** 的总回报 $G_t$ 进行的加权平均。一个轨迹是一个序列 $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, R_{t+2}, \dots$。

将期望只展开一步，可以更清晰地看到这个过程：

从状态 $s$ 开始，智能体首先根据策略 $\pi(a|s)$ 选择一个动作 $a$。然后，环境根据状态转移概率 $P(s'|s, a)$ 转移到一个新的状态 $s'$，并给出一个奖励 $R(s, a, s')$。这个过程不断重复。

因此，期望 $\mathbb{E}_{\pi}$ 实际上是综合了所有这些可能性的结果。如果我们只看第一步，它可以被分解为对动作 $a$ 和后继状态 $s'$ 的期望：

$V^{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] = \sum_{a \in \mathcal{A}} \pi(a|s) \left( \sum_{s' \in \mathcal{S}} P(s'|s, a) \left[ R(s, a, s') + \gamma \mathbb{E}_{\pi}[G_{t+1} | S_{t+1} = s'] \right] \right)$

这里的内层期望 $\mathbb{E}_{\pi}[G_{t+1} | S_{t+1} = s']$ 就是我们定义的 $V^{\pi}(s')$。这正是贝尔曼方程的核心思想：用后继状态的价值来表示当前状态的价值。

#### 2. 策略 $\pi$ 的范围

在 $V^{\pi}(s)$ 的定义中，策略 $\pi$ 是指智能体在**整个马尔可夫决策过程中，在所有时间步都将遵循的同一个策略**。

它不是一个“一步”策略，而是一个完整的、固定的行为准则。当我们写下 $V^{\pi}(s)$ 时，我们是在评估“如果从状态 $s$ 开始，并且在未来的每一步都按照策略 $\pi$ 来行动，那么我们期望获得的总回报是多少？”。

因此，这个 $\pi$ 定义了整个马尔可夫链上的行为模式，价值函数 $V^{\pi}$ 是基于这个完整的行为模式计算出来的。

### 4. 贝尔曼期望方程推导

#### a. 推导 $V^{\pi}(s)$

1.  从状态价值函数的定义开始：
    $V^{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s]$

2.  将 $G_t$ 的定义展开：
    $V^{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]$

3.  利用期望的线性性质：
    $V^{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} | S_t = s] + \gamma \mathbb{E}_{\pi}[G_{t+1} | S_t = s]$

4.  **详细展开期望项**：
    为了从第 3 步过渡到第 4 步，我们需要使用**全期望定律 (Law of Total Expectation)**。这里的期望 $\mathbb{E}_{\pi}$ 是对未来所有不确定性的综合考量，具体来说，它包括了策略 $\pi$ 将选择的**动作**和环境动态 $P$ 将导致的**后继状态**。

    我们可以分步来拆解这个期望：

    -   **首先，对所有可能的动作 $A_t$ 求期望。**
        在状态 $s$ 时，智能体将根据策略 $\pi(a|s)$ 随机选择一个动作 $A_t=a$。因此，我们可以先对所有可能的动作 $a$ 求加权平均（即期望）：
        $V^{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s] = \sum_{a \in \mathcal{A}} \pi(a|s) \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]$

    -   **其次，对所有可能的后继状态 $S_{t+1}$ 求期望。**
        在确定了动作 $a$ 之后，环境将根据转移概率 $P(s'|s, a)$ 随机进入一个后继状态 $S_{t+1}=s'$。因此，我们对内层的期望再次进行分解：
        $\mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] = \sum_{s' \in \mathcal{S}} P(s'|s, a) \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$

    -   **然后，简化最内层的期望。**
        当 $S_t=s, A_t=a, S_{t+1}=s'$ 都确定后：
        -   即时奖励 $R_{t+1}$ 的期望就是其确定值 $R(s, a, s')$。
        -   根据马尔可夫性质，未来总回报 $G_{t+1}$ 的期望只与当前状态 $S_{t+1}=s'$ 有关。所以 $\mathbb{E}_{\pi}[G_{t+1} | \dots, S_{t+1}=s'] = \mathbb{E}_{\pi}[G_{t+1} | S_{t+1}=s']$，而这正是 $V^{\pi}(s')$ 的定义。
        因此，最内层的期望可以被写为：$R(s, a, s') + \gamma V^{\pi}(s')$。

    -   **最后，组合所有部分。**
        将简化的结果代回，我们就得到了完整的展开式，这也就是**状态价值函数的贝尔曼期望方程**:

    $V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}} P(s'|s, a) \left[ R(s, a, s') + \gamma V^{\pi}(s') \right]$

    也可以写作期望的形式：

    $V^{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma V^{\pi}(S_{t+1}) | S_t = s]$

#### b. 推导 $Q^{\pi}(s, a)$

对 $Q^{\pi}(s, a)$ 的推导过程与 $V^{\pi}(s)$ 类似，但更直接，因为它已经以一个确定的动作 $a$ 为前提。

1.  **从定义开始**：
    $Q^{\pi}(s, a) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]$

2.  **展开总回报 $G_t$**：
    $Q^{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]$

3.  **详细展开期望项**：
    在 $Q^{\pi}(s, a)$ 的定义中，条件是 $S_t = s$ 和 $A_t = a$，这意味着当前状态和要执行的动作都已确定。因此，下一步的唯一不确定性来自于环境会转移到哪个后继状态 $S_{t+1}=s'$。这种不确定性由状态转移概率 $P(s'|s, a)$ 描述。

    因此，我们可以直接应用全期望定律，对所有可能的后继状态 $s'$ 求期望：
    $\mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] = \sum_{s' \in \mathcal{S}} P(s'|s, a) \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$

4.  **简化最内层的期望**：
    当 $S_t=s, A_t=a, S_{t+1}=s'$ 全部确定后：
    -   即时奖励 $R_{t+1}$ 的期望就是其确定值 $R(s, a, s')$。
    -   未来总回报 $G_{t+1}$ 的期望 $\mathbb{E}_{\pi}[G_{t+1} | \dots, S_{t+1}=s']$ 等于 $V^{\pi}(s')$。
    因此，最内层的期望可以被写为：$R(s, a, s') + \gamma V^{\pi}(s')$。

5.  **组合得到最终方程**：
    将简化的结果代回，我们就得到了**动作价值函数的贝尔曼期望方程**的第一种形式:

    $Q^{\pi}(s, a) = \sum_{s' \in \mathcal{S}} P(s'|s, a) \left[ R(s, a, s') + \gamma V^{\pi}(s') \right]$

    这个方程也可以进一步用 $Q$ 函数自身来表示。我们知道 $V^{\pi}(s') = \sum_{a' \in \mathcal{A}} \pi(a'|s') Q^{\pi}(s', a')$，代入上式可得第二种形式：

    $Q^{\pi}(s, a) = \sum_{s' \in \mathcal{S}} P(s'|s, a) \left[ R(s, a, s') + \gamma \sum_{a' \in \mathcal{A}} \pi(a'|s') Q^{\pi}(s', a') \right]$

    这两种形式都是贝尔曼期望方程的重要体现。$

### 总结

我们得到了两个核心的贝尔曼期望方程：

1.  **状态价值函数**:
    $V^{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma V^{\pi}(S_{t+1}) | S_t = s]$

2.  **动作价值函数**:
    $Q^{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma Q^{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]$

这两个方程是强化学习中许多算法的理论基石。

---

## 贝尔曼最优方程 (Bellman Optimality Equation)

贝尔曼最优方程是强化学习中的另一个核心概念。它不像期望方程那样描述一个给定策略 `π` 的价值，而是描述在所有可能的策略中，能获得的最大价值，即“最优价值”。

### 1. 最优价值函数 (Optimal Value Functions)

首先，我们定义最优价值函数，它们代表了在某个状态或状态-动作对上可能达到的最大化未来回报。

- **最优状态价值函数 (Optimal State-Value Function)**, $V^*(s)$，是在所有策略中，针对状态 `s` 能产生的最大价值：
  $V^*(s) = \max_{\pi} V^{\pi}(s)$

- **最优动作价值函数 (Optimal Action-Value Function)**, $Q^*(s, a)$，是在所有策略中，针对状态-动作对 `(s, a)` 能产生的最大价值：
  $Q^*(s, a) = \max_{\pi} Q^{\pi}(s, a)$

这些函数定义了智能体在环境中能够达到的理论性能上限。

### 2. V*(s) 的推导

一个状态 `s` 的最优价值 $V^*(s)$，等于从这个状态出发，采取“最优”的动作后，所能获得的期望回报。这个“最优”的动作，就是那个能够引导至最大 $Q^*(s, a)$ 的动作。

因此，$V^*(s)$ 必须等于从所有可能的动作 `a` 中选择，所能得到的最大 $Q^*(s, a)$ 值：

$V^*(s) = \max_{a \in \mathcal{A}} Q^*(s, a)$

现在，我们来展开 $Q^*(s, a)$。$Q^*(s, a)$ 的定义是：在状态 `s` 执行动作 `a`，然后**遵循最优策略**所能得到的期望回报。这与贝尔曼期望方程类似，但关键区别在于，我们假设后续的每一个状态 `s'` 都将获得其最优价值 $V^*(s')$。

$Q^*(s, a) = \mathbb{E}[R_{t+1} + \gamma V^*(S_{t+1}) | S_t=s, A_t=a]$

对所有可能的下一个状态 `s'` 求期望，我们可以得到：

$Q^*(s, a) = \sum_{s' \in \mathcal{S}} P(s'|s, a) \left[ R(s, a, s') + \gamma V^*(s') \right]$

最后，将这个 $Q^*(s, a)$ 的表达式代入到 $V^*(s)$ 的方程中，我们得到：

$V^*(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} P(s'|s, a) \left[ R(s, a, s') + \gamma V^*(s') \right]$

这就是**状态价值函数 $V^*$ 的贝尔曼最优方程**。它表明，一个状态的最优价值，等于从该状态出发，选择最佳动作后所能获得的期望回报。

### 3. Q*(s, a) 的推导

$Q^*(s, a)$ 的推导更为直接。它代表在状态 `s` 采取动作 `a`，然后永远遵循最优策略所能获得的价值。执行动作 `a` 后，系统会转移到下一个状态 `s'`，而从 `s'` 开始遵循最优策略，能获得的价值就是 $V^*(s')$。

根据定义：
$Q^*(s, a) = \mathbb{E}[R_{t+1} + \gamma V^*(S_{t+1}) | S_t=s, A_t=a]$

展开对下一状态 `s'` 的期望：
$Q^*(s, a) = \sum_{s' \in \mathcal{S}} P(s'|s, a) \left[ R(s, a, s') + \gamma V^*(s') \right]$

我们已经知道 $V^*(s') = \max_{a' \in \mathcal{A}} Q^*(s', a')$，将此关系代入上式：

$Q^*(s, a) = \sum_{s' \in \mathcal{S}} P(s'|s, a) \left[ R(s, a, s') + \gamma \max_{a' \in \mathcal{A}} Q^*(s', a') \right]$

这就是**动作价值函数 $Q^*$ 的贝尔曼最优方程**。它将一个状态-动作对的最优价值，用所有可能的后继状态-动作对的最优价值来表示。

